<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>Lattice PCIe DMA Demos: Introduction</title>
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.3.9.1 -->
<div class="qindex"><a class="qindex" href="main.html">Main&nbsp;Page</a> | <a class="qindex" href="files.html">File&nbsp;List</a> | <a class="qindex" href="globals.html">File&nbsp;Members</a> | <a class="qindex" href="pages.html">Related&nbsp;Pages</a></div>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
</head>
<body>
    <H2>DMA Demo Configuration</H2>
Direct Memory Access (DMA) is a method of transferring data from one memory mapped device to another.  The data is transferred by a dedicated device that performs the bus cycle (memory reads and writes).  The CPU is not involved in the actual data movement.  Using a dedicated DMA device frees the CPU to do other operations and also shortens the transfer time.  If the CPU had to move the data, it would be done in a software loop which requires fetching, decoding and executing each instruction involved in the loop.  This could easily expand to 10 or more instruction cycles per datum moved.  A DMA engine could perform the same datum move operation in 1 to 3 bus clocks (depending on bus architecture).  
<p>
In modern PC systems the DMA engine, the device responsible for performing the bus cycles to implement the transfer, is located on the add-in card.   This is known as Bus Master DMA and is the preferred method of operation.  The PCI bus is being phased out and replaced with the PCIe bus.  In order to take advantage of the high bandwidth that PCIe offers, DMA is used to transfer the data between the add-in card and the system memory.  The Lattice SGDMAC IP works in conjunction with the Lattice PCIe IP core to transport the data.
<p>
The Lattice Semiconductor Scatter-Gather DMA IP core, together with the PCIe IP core, demonstrates moving data between the Lattice FPGA and PC system memory using a Lattice PCI Express evaluation board.  The board uses the PCI Express link as both control (setup and operation of the core) and data path (DMA to/from PC system memory).  The PC provides the test platform (power, run-time environment) and the user interface.  A PC platform is used because currently PCs are the only readily available, economical and standard platform utilizing PCIe. A device driver provides the interface to the board’s register and memory space.  Application software uses the driver to setup and configure the DMA engine, execute it and verify the results.  The demo system is illustrated in the following block diagram.
<p>

<img align=center src="./hdw_diag.gif" alt="Demo Block Diagram">

<p>

The demo hardware has the following objectives:
<ul>
    <li>Give a reference design for using the PCIe and SGDMA IP cores
    <li>Perform actual DMA transfers over the PCIe bus at an optimal rate
    <li>Provide counters and timers to measure performance
    <li>Provide a platform for demonstration and experimentation
</ul>

The demo application software has the following objectives:
<ul>
    <li>Demonstrate accessing, configuring and operating the PCIe and SGDMA IP cores
    <li>Verify proper operation (ensure all DMA data is transferred from source to destination un-corrupted)
    <li>Demonstrate driver and system programming so users can extend software for their own particular system needs.
    <li>System memory allocation and transformation into Scatter-Gather lists.
    <li>Interrupt handling - ISRs and DPCs
</ul>



    <H2>SGDMA Operations</H2>

    Hardware devices perform Direct Memory Access by initiating read/write bus transactions.
    Direct Memory Access means transferring data to/from system memory directly, without involving the CPU.  
    Bus Master DMA means the device (the PCIe Core on the Eval Board) is controlling the bus and doing the data transfers.
    In order to perform the transfer, an address is needed and a length.  The SGDMA is configured by the software driver.
    The addresses known to software for describing a buffer's location in memory are only relevant in the domain of the CPU.
    The CPU (and software) view memory as virtual 2 GB address spaces per process - the DMA needs physical memory addresses.
    When software allocates a large buffer of memory, the memory manager finds the number of required free pages (4KB per page)
    in system memory and makes them appear contiguous to software via virtual memory translation tables in hardware.
    A 1MB buffer alocated by user software appears contiguous to the software, but in reality is scattered throughout
    physical system memory in discontinuous 4KB chunks.  The magic of virtual memory makes it appear contiguous to software.
    <p>
    The SGDMA needs physical addresses to put on the bus and needs contiguous memory.  In a simple flat memory architecture, 
    the SGDMA could just take a starting address and a length of 1MB and transfer all data in one continuous operation.
    In vitrual memory machines, the kernel and memory manager need to be enlisted at the driver level to create a map
    of the virtual memory to physical pages.  This mapping is known as a Scatter-Gather list.
    The Scatter-Gather List maps virtual memory to physical page addresses.   The device driver uses the Scatter-Gather list entries
    to program the buffer descriptors.  Each buffer descriptor is programmed with the physical memory address and the length
    (usually one page, 4096 bytes).  When the SGDMA channel is activated it reads the linked list of buffer descriptors and
    moves the data to that address, and then moves to the next buffer descriptor and next address, until the end of the list
    is reached.  The following figure illustrates this operation.
    <p> 

    <img align=center src="./sgdma_list.gif" alt="Scatter Gather List">
    <p>
    The buffer descriptors (BD[], shown on left in blue) have their destination addresses programmed to the start of
    the physical pages in memory.  These pages (shown in green) may not be contiguous or sequential in memory.
    The memory manager in the PC hardware use the Scatter Gather List (SG List, shown in orange) to make this
    set of pages appear contiguous to the application running in user space (Virtual Memory mode).
    <p>
    The SGDMA off-loads the processor and kernel by having the ability to perform this scattering of contiguous data
    (memory on the eval board) to arbitrary memory pages, or for reading, to gather a set of discontinuous memory pages
    into a contiguous memory on the eval board.

    <H2>More Info</H2>
    See the <B>Code</B> links to the DMATest.cpp, ColorBars.cpp and ImageMove.cpp files.
    The source code is the best documentation of what is really going on behind the
    scenes in the demos.

    
    <H2>Running the Demos</H2>

    <H3>Starting a Demo</H3>
    The demos are started from desk-top icons or the Start menu.  Navigate to the LatticePCIeDMA program group.  There are
    a number of programs to select from:
    <ul>
	<li> DMATest - run IP and driver tests in a console window
	<li> ColorBars - run a graphical DMA demo (image data from FPGA moved to memory and displayed)
	<li> ImageMove - run a graphical DMA demo (image data read from memory, modified, moved back to memory and displayed)
	<li> Scan Boards - run PCIe Eval Board scan utility to select which board to execute demo on
	<li> User Guide - display this document
	<li> Uninstall PCIeDMA - uninstall this demo directory, icons and all programs
    </ul>
    <p>

    The 16 Segment LED displays the real-time interrupt processing during execution of the ColorBars and ImageMove.
    The inner 8 segments are the lower 8 bits of the ISR routine counter.  The outer 8 segments are the lower 8 bits of
    the DPC routine counter, where real processing is done.  All segments (inner and outer) should be changing at a rapid rate during
    demo operation (interrupts after each DMA transfer) indicating that the hardware is operating and interrupts are
   being serviced. 
    <p>
    A demo can fail with an error box if:
    <ul>
	<li>The board is not recognized by hardware or the OS
	<li>The driver is not loaded (bitstream not PCIe demo)
	<li>The PCIe link is not a x4
	<li>Driver can not access registers
	<li>Application or driver can not verify IP register IDs
	<li>Another demo is running
    </ul>

    <p>
    <b><i>Note:</i></b>The DMATest  program requires a large scroll buffer for recording all
    the test output (approx 2000 lines).  The default console window may not have enough visible lines.
    Increase the amount of lines stored in the scroll buffer to capture all the output from the tests
    or redirect output to a file. 


    <H3>Run One Demo At A Time</H3>
    Do not run more than one demo at a time.  The ImageMove and ColorBars demos can not be run at the same time because they are mutually exclusive.  Each needs DMA channels in the SGDMA.  The driver marks channels as in-use once a demo "opens" the channels.  Starting another demo will fail when it attempts to open the same channels.  The DMATest demo does not use the channels provided by the driver. It instead uses direct register reads and writes to verify access to the IP.  It writes patterns into all SGDMA registers and buffer descriptors to verify programmability.
    DMATest can be started while one of the other demos is operating.  If this occurs, a crash is possible since the DMATest application is directly modifying SGDMA register values while real transfer operations are using them. <B>It is up to the user to ensure the DMATest is never run while other DMA operations are executing.</b> The DMATest is intended for stand-alone verification of the IP, not a demonstration of DMA operations.

    <H3>Multiple Board Support</H3>
    The demos support executing on any number of installed PCIe Eval boards.  The board must obviously have a valid PCIeDMA demo bitstream loaded in the SPI flash.
    The eval boards are enumerated when the demo is started using a Lattice PCIe scan.exe utility program.  If multiple are identified, the user is given a choice to select from using a simple text menu.  Enter a 1 for the first board, a 2 for the second board, etc.  The demo will then execute using that eval board.
    <p>
    Multiple boards can each execute their own demo.  If two eval boards are installed, they can each be running the ColorBars demo.


    <H2>Known Issues and Limitations</H2>
    <ul>
	<li>Only 2 SGDMA channels are used.  One for reading from the PC (MRd requests) and one for writing to the PC (MWr). This means
	only 1 demo can run at a time on a board.  Also the read rate is not optimal.  If more channels were implemented the read requests
	could be interleaved among them, increasing throuhput.
	<li>You can not DMA between eval baords.  The Wishbone completer only supports 1 DW TLPs.  This path is used for the control plane.
	The software does not support transfer between the BARs of two different boards.
	<li>An issue with the SGDMA IP Core operating in FIFO mode has been identified and is under investigation.  
	The issue, as implemented in this design, shows up as the SGDMA reading one too many words per burst.
	The extra read word is dropped, and lost from the FIFO.
	<li>The driver does not graciously abort system requests that should timeout.  A timeout for IRPs was not implemented. 
	The demo application will appear frozen and will not timeout if it does not get responses from the eval board.
    </ul>
    
    <H2>FAQs</H2>
    <H3>What does this show?</H3>
    The demos show the Lattice PCIe and SGDMA solution operating in a real system (true Scatter Gather DMA over
    the PCIe bus, using interrupts and User Space virtual memory as the source/destination of the transfer).  
    The data is moved over the PCIe bus by means of a Master DMA controller implemented in the FPGA.
    The data is displayed in graphics windows.  A graphical display was chosen as quick, easy way to show large
    (1MB) data transfers in real-time.  DMA operations are shown by way of the changing images in the
    demo windows. The contents of the DMA transfer are the byte values used to draw the pixels.

    <H3>What is the transfer rate?</H3>
    The transfer rate can be obtained from the frame rate displayed in the title bar of the windows.  
    The ColorBars demo moves 1MB of data in each DMA operation.  This is one frame of video.  The frame rate is controlled by
    the OpenGL library and video hardware.  Frame rate is usually a function of the monitor refresh rate 
    (a new frame buffer is displayed each cycle).  If the monitor refresh rate is 60Hz, then 60 frames will be displayed a second.  
    This means the DMA will be moving 60 MB/sec.   As long as the DMA transfer keeps up with this rate,
    the application requirements are satisfied.
    <p>
    Put another way, these demos show real-world system DMA operations, not just raw PCIe thruput numbers.  A number of system level
    factors influence the transfer rate, such as system load, memory load, system hardware, 
    performance of other libraries used by the application, and the application's architecture.
    See the Lattice PCIe Thruput demo for raw PCIe transfer rate numbers.

    <H3>Is this demo doing real hardware DMA?</H3>
    Yes.  The application software allocates memory in user space.  It uses the device driver to construct a
    scatter-gather list mapping all the virtual pages to physical addresses and lengths.  The SGDMA buffer descriptors
    are programmed by the driver with the list information and the channel is configured.  The channel is started by the driver.
    The SGDMA operates autonomously until all pages in the list have been transfered.  It then interrupts the CPU.  
    The driver's ISR sees if more data needs to be moved, or if it is completed, and then cleans up.  
    Control is then returned to the OS API call
    that initiated the transfer (ReadFile or WriteFile) and when it returns to the user, the user's buffer has been
    either moved to the Eval Board, or loaded by the Eval Board.  All data movement is done entirely by the IP
    on the Eval Board.





</body>
</html>
 <hr size="1"><address style="align: right;"><small>Generated on Wed Jul 16 12:05:28 2008 for Lattice PCIe DMA Demos by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.3.9.1 </small></address>
</body>
</html>
